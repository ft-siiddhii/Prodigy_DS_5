{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <div style=\"text-align: center; background-color:#0084b4; font-family:Impact; color: white; padding: 20px; line-height: 1;border-radius:20px\">Twitter Sentiment Analysis üïäÔ∏è</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"width:100%;text-align:center\"> \n<img align=middle src = \"https://ichef.bbci.co.uk/news/976/cpsprodpb/13B2F/production/_127678608_gettyimages-1244636244.jpg\" width=\"500px\">\n</div>","metadata":{"execution":{"iopub.status.busy":"2023-08-13T14:08:37.002386Z","iopub.execute_input":"2023-08-13T14:08:37.002789Z","iopub.status.idle":"2023-08-13T14:08:37.020888Z","shell.execute_reply.started":"2023-08-13T14:08:37.002759Z","shell.execute_reply":"2023-08-13T14:08:37.019687Z"}}},{"cell_type":"markdown","source":"<div style='background-color:#1dcaff;color:white;padding:4px;border-radius:25px;font-family:georgia'>\n<h3 style='color:white;font-family:Impact'>&nbsp About Dataset üìë</h3>\n</div>\n\n<div style='padding:6px; font-size:16'>\n    <p>The dataset consists of message, entity, and sentiment in Twitter. There are three classes in the dataset: positive, negative, and neutral. The messages that are not relevant to the entity should be regarded as neutral.</p>\n    <ul>\n        <li><mark><b>Tweet ID:</b></mark> ID of Tweet\n        </li>\n        <li><mark><b>Entity:</b></mark> Entity that Tweet talks about\n        </li>\n       <li><mark><b>Sentiment:</b></mark> Sentiment of the tweet text regarding the entity\n           <br/>&nbsp Positive, Negative, Neutral, Irrelevant\n        </li>\n        <li><mark><b>Tweet Content:</b></mark> Tweet Text\n        </li>\n    </ul>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style='background-color:#1dcaff;color:white;padding:4px;border-radius:25px;font-family:georgia'>\n<h3 style='color:white;font-family:Impact'>&nbsp Goal of the Project üôá‚Äç‚ôÄÔ∏è</h3>\n</div>\n\n<div style='padding:6px'>\n    <p>The goal of the project is to üìä <mark>explore</mark> data (EDA Analysis), ‚öôÔ∏è <mark>perform NLP preprocessing</mark>, and ü§ñ <mark>perform ML </mark> to judge the sentiment of the message about the entity.\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style='background-color:#1dcaff;color:white;padding:4px;border-radius:25px;font-family:georgia'>\n    <h3 style='color:white;font-family:Impact'>&nbsp Table of Contents üßö</h3>\n</div>\n<ul style='padding:6px'>\n    <a href='#1'><b>1. Import Libraries üìö</b><br/></a>\n    <a href='#2'><b>2. Exploratory Data Analysis üìä</b></a>\n    <ul>\n        <a href='#2.1'><b>2.1 Sentiment Analysis</b><br/></a>\n            <ul>\n                <a href='#2.1.1'>2.1.1 Distribution of Sentiment<br/></a>\n                <a href='#2.1.2'>2.1.2 Distribution of Entity<br></a>\n                <a href='#2.1.3'>2.1.3 Sentiment Distribution in Top 3 Entities<br></a>\n            </ul>\n        <a href='#2.2'><b>2.2 Text Analysis with NLP Preprocessing<br/></b></a>\n            <ul>\n                <a href='#2.2.1'>2.2.1 NLP Preprocessing<br/></a>\n                <a href='#2.2.2'>2.2.2 Positive Sentiment Text Distribution<br/></a>\n                <a href='#2.2.3'>2.2.3 Negative Sentiment Text Distribution<br></a>\n                <a href='#2.2.4'>2.2.4 Neutral Sentiment Text Distribution<br></a>\n            </ul>\n    </ul>\n    <a href='#3'><b>3. ML Pipeline Modelling ü§ñ</b></a>\n</ul>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n# <div style=\"text-align: center; background-color: #00aced; font-family:Impact; color: white; padding: 14px; line-height: 1;border-radius:20px\">1. Import Libraries üìö</div>","metadata":{}},{"cell_type":"code","source":"## Remove Warnings ## \nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n## DATA ## \nimport numpy as np \nimport pandas as pd \nimport re\n\n## NLP ##\nimport nltk \nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer \n\n## Visualization ## \nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt  \nfrom plotly.subplots import make_subplots\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\n\n## ML Modelling ## \nfrom sklearn.pipeline import Pipeline \nfrom sklearn.feature_extraction.text import TfidfVectorizer \nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.metrics import accuracy_score \nfrom sklearn.model_selection import GridSearchCV","metadata":{"execution":{"iopub.status.busy":"2023-08-26T10:36:54.217902Z","iopub.execute_input":"2023-08-26T10:36:54.21816Z","iopub.status.idle":"2023-08-26T10:36:55.138556Z","shell.execute_reply.started":"2023-08-26T10:36:54.218135Z","shell.execute_reply":"2023-08-26T10:36:55.137258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col_names = ['ID', 'Entity', 'Sentiment', 'Content']\ntrain_df = pd.read_csv('/kaggle/input/twitter-entity-sentiment-analysis/twitter_training.csv', names=col_names)\ntest_df = pd.read_csv('/kaggle/input/twitter-entity-sentiment-analysis/twitter_validation.csv', names=col_names)","metadata":{"execution":{"iopub.status.busy":"2023-08-26T10:39:02.577105Z","iopub.execute_input":"2023-08-26T10:39:02.577595Z","iopub.status.idle":"2023-08-26T10:39:02.758468Z","shell.execute_reply.started":"2023-08-26T10:39:02.577557Z","shell.execute_reply":"2023-08-26T10:39:02.756912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-26T10:39:02.761482Z","iopub.execute_input":"2023-08-26T10:39:02.761791Z","iopub.status.idle":"2023-08-26T10:39:02.779496Z","shell.execute_reply.started":"2023-08-26T10:39:02.761763Z","shell.execute_reply":"2023-08-26T10:39:02.777408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-08-26T10:39:02.781567Z","iopub.execute_input":"2023-08-26T10:39:02.781918Z","iopub.status.idle":"2023-08-26T10:39:02.827874Z","shell.execute_reply.started":"2023-08-26T10:39:02.781888Z","shell.execute_reply":"2023-08-26T10:39:02.826184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since there are <b>686</b> null values in content (text), I will drop them. ","metadata":{}},{"cell_type":"code","source":"train_df.dropna(subset=['Content'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-26T10:39:02.829234Z","iopub.execute_input":"2023-08-26T10:39:02.829569Z","iopub.status.idle":"2023-08-26T10:39:02.86734Z","shell.execute_reply.started":"2023-08-26T10:39:02.829543Z","shell.execute_reply":"2023-08-26T10:39:02.865366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['Sentiment'] = train_df['Sentiment'].replace('Irrelevant', 'Neutral')\ntest_df['Sentiment'] = test_df['Sentiment'].replace('Irrelevant', 'Neutral')","metadata":{"execution":{"iopub.status.busy":"2023-08-26T10:39:02.869623Z","iopub.execute_input":"2023-08-26T10:39:02.869939Z","iopub.status.idle":"2023-08-26T10:39:02.884454Z","shell.execute_reply.started":"2023-08-26T10:39:02.869909Z","shell.execute_reply":"2023-08-26T10:39:02.883584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I will replace <b>irrelevant</b> to <b>neutral</b>.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n# <div style=\"text-align: center; background-color: #00aced; font-family:Impact; color: white; padding: 14px; line-height: 1;border-radius:20px\">2. Exploratory Data Analysis üìä</div>","metadata":{}},{"cell_type":"markdown","source":"In this section, I will first <b>analyze sentiment</b> distribution and sentiment distribution by top 3 entity, and then <b>NLP preprocess texts</b>, and lastly <b>visualize text distribution</b> by each sentiment. ","metadata":{}},{"cell_type":"markdown","source":"<div id='2.1' style='background-color:#51C7FF;text-align:center;padding:4px;border-radius:25px'>\n    <h3 style='color:white;font-family:Impact'>2.1 Sentiment Analysis </h3>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div id='2.1.1' style='background-color:#9FDFFF;padding:2px;border-radius:25px'>\n    <h4 style='font-family:Impact; color:black'>&nbsp 2.1.1 Distribution of Sentiment</h4>\n</div>","metadata":{}},{"cell_type":"code","source":"sentiment_counts = train_df['Sentiment'].value_counts().sort_index()\n\nsentiment_labels = ['Negative', 'Neutral', 'Positive']\nsentiment_colors = ['red', 'grey', 'green']\n\nfig = go.Figure(data=[go.Pie(labels=sentiment_counts.index, \n                             values=sentiment_counts.values,\n                             textinfo='percent+value+label',\n                             marker_colors=sentiment_colors,\n                             textposition='auto',\n                             hole=.3)])\n\nfig.update_layout(\n    title_text='Sentiment Distribution',\n    template='plotly_white',\n    xaxis=dict(\n        title='Sources',\n    ),\n    yaxis=dict(\n        title='Number of Posts in Twitter',\n    )\n)\n\nfig.update_traces(marker_line_color='black', \n                  marker_line_width=1.5, \n                  opacity=0.8)\n \nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-26T10:39:02.886265Z","iopub.execute_input":"2023-08-26T10:39:02.886645Z","iopub.status.idle":"2023-08-26T10:39:03.259477Z","shell.execute_reply.started":"2023-08-26T10:39:02.886606Z","shell.execute_reply":"2023-08-26T10:39:03.257974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are <b>41.9%</b> of neutral sentiment texts about entity, <b>30.2%</b> of negative sentiment texts about entity, and <b>27.9%</b> of positive sentiment texts about entity.","metadata":{}},{"cell_type":"markdown","source":"<div id='2.1.2' style='background-color:#9FDFFF;padding:2px;border-radius:25px'>\n    <h4 style='font-family:Impact; color:black'>&nbsp 2.1.2 Distribution of Entity</h4>\n</div>","metadata":{}},{"cell_type":"code","source":"top10_entity_counts = train_df['Entity'].value_counts().sort_values(ascending=False)[:10]\n\nfig = px.bar(x=top10_entity_counts.index, \n             y=top10_entity_counts.values,\n             color=top10_entity_counts.values,\n             text=top10_entity_counts.values,\n             color_continuous_scale='Blues')\n\nfig.update_layout(\n    title_text='Top 10 Twitter Entity Distribution',\n    template='plotly_white',\n    xaxis=dict(\n        title='Entity',\n    ),\n    yaxis=dict(\n        title='Number of Posts in Twitter',\n    )\n)\n\nfig.update_traces(marker_line_color='black', \n                  marker_line_width=1.5, \n                  opacity=0.8)\n \nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-26T10:39:03.261365Z","iopub.execute_input":"2023-08-26T10:39:03.261845Z","iopub.status.idle":"2023-08-26T10:39:03.391568Z","shell.execute_reply.started":"2023-08-26T10:39:03.26181Z","shell.execute_reply":"2023-08-26T10:39:03.390567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are about <b>same</b> amount of data for each entity. <mark>MaddenNFL, LeagueOfLegends, CallOfDuty</mark> are 3 most distributed entities in the dataset.","metadata":{}},{"cell_type":"markdown","source":"<div id='2.1.3' style='background-color:#9FDFFF;padding:2px;border-radius:25px'>\n    <h4 style='font-family:Impact; color:black'>&nbsp 2.1.3 Sentiment Distribution in Top 3 Entities</h4>\n</div>","metadata":{}},{"cell_type":"code","source":"top3_entity_df = train_df['Entity'].value_counts().sort_values(ascending=False)[:3]\ntop3_entity = top3_entity_df.index.tolist()\nsentiment_by_entity = train_df.loc[train_df['Entity'].isin(top3_entity)].groupby('Entity')['Sentiment'].value_counts().sort_index()\n\nsentiment_labels = ['Negative', 'Neutral', 'Positive']\nsentiment_colors = ['red', 'grey', 'green']\n\nrow_n = 1\ncol_n = 3\n\nfig = make_subplots(rows=row_n, cols=col_n, \n                    specs=[[{'type':'domain'}, {'type':'domain'}, {'type':'domain'}]],\n                    subplot_titles=top3_entity)\n\nfor i, col in enumerate(top3_entity):\n    fig.add_trace(\n        go.Pie(labels=sentiment_labels, \n                values=sentiment_by_entity[col].values, \n                textinfo='percent+value+label',\n                marker_colors=sentiment_colors,\n                textposition='auto',\n                name=col),\n            row=int(i/col_n)+1, col=int(i%col_n)+1)\n    \nfig.update_traces(marker_line_color='black', \n                  marker_line_width=1.5, \n                  opacity=0.8)\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-26T10:39:03.392889Z","iopub.execute_input":"2023-08-26T10:39:03.39325Z","iopub.status.idle":"2023-08-26T10:39:03.457501Z","shell.execute_reply.started":"2023-08-26T10:39:03.39322Z","shell.execute_reply":"2023-08-26T10:39:03.456403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are <b>71.3%</b> negative sentiment tweets about MaddenNFL, <b>47.5%</b> neutral sentiment tweets about LeagueOfLegends, <b>44.1%</b> neutral sentiment tweets about CallOfDuty. ","metadata":{}},{"cell_type":"markdown","source":"<div id='2.2' style='background-color:#51C7FF;text-align:center;padding:4px;border-radius:25px'>\n    <h3 style='color:white;font-family:Impact'>2.2 Text Analysis with NLP Preprocessing </h3>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div id='2.2.1' style='background-color:#9FDFFF;padding:2px;border-radius:25px'>\n    <h4 style='font-family:Impact; color:black'>&nbsp 2.2.1 NLP Preprocessing</h4>\n</div>","metadata":{}},{"cell_type":"markdown","source":"In this section, I will perform <mark>NLP Preprocessing</mark> and <mark>visualize texts</mark> for each sentiment. <hr/>\nPreprocessing Functions Explanations:\n- <mark><b>get_all_string:</b></mark> this function returns all strings in one sentence given a text series\n- <mark><b>get_word:</b></mark> this function returns list of words given a sentence \n- <mark><b>remove_stopword:</b></mark> this function removes stopwords like \"the\", \"is\", \"and\", and etc\n- <mark><b>lemmatize_word:</b></mark> this function lemmatizes the word (i.e. \"Caring\" --> \"Care\") \n- <mark><b>create_freq_df:</b></mark> this function returns the frequency dataframe given the list of words","metadata":{}},{"cell_type":"code","source":"def get_all_string(sentences): \n    sentence = ''\n    for words in sentences:\n        sentence += words\n    sentence = re.sub('[^A-Za-z0-9 ]+', '', sentence)\n    sentence = re.sub(r'http\\S+', '', sentence)\n    sentence = sentence.lower()\n    return sentence \n\ndef get_word(sentence):\n    return nltk.RegexpTokenizer(r'\\w+').tokenize(sentence)\n\ndef remove_stopword(word_tokens):\n    stopword_list = stopwords.words('english')\n    filtered_tokens = []\n    \n    for word in word_tokens:\n        if word not in stopword_list: \n            filtered_tokens.append(word) \n    return filtered_tokens \n\ndef lemmatize_words(filtered_tokens):\n    lemm = WordNetLemmatizer() \n    cleaned_tokens = [lemm.lemmatize(word) for word in filtered_tokens]\n    return cleaned_tokens","metadata":{"execution":{"iopub.status.busy":"2023-08-26T10:39:03.45932Z","iopub.execute_input":"2023-08-26T10:39:03.459622Z","iopub.status.idle":"2023-08-26T10:39:03.468314Z","shell.execute_reply.started":"2023-08-26T10:39:03.459599Z","shell.execute_reply":"2023-08-26T10:39:03.466659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_freq_df(cleaned_tokens): \n    fdist = nltk.FreqDist(cleaned_tokens)\n    freq_df = pd.DataFrame.from_dict(fdist, orient='index')\n    freq_df.columns = ['Frequency']\n    freq_df.index.name = 'Term'\n    freq_df = freq_df.sort_values(by=['Frequency'], ascending=False)\n    freq_df = freq_df.reset_index()\n    return freq_df","metadata":{"execution":{"iopub.status.busy":"2023-08-26T10:39:03.47053Z","iopub.execute_input":"2023-08-26T10:39:03.470928Z","iopub.status.idle":"2023-08-26T10:39:03.484506Z","shell.execute_reply.started":"2023-08-26T10:39:03.470893Z","shell.execute_reply":"2023-08-26T10:39:03.483284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess(series):\n    all_string = get_all_string(series)\n    words = get_word(all_string)\n    filtered_tokens = remove_stopword(words)\n    cleaned_tokens = lemmatize_words(filtered_tokens)\n    return cleaned_tokens","metadata":{"execution":{"iopub.status.busy":"2023-08-26T10:39:03.488292Z","iopub.execute_input":"2023-08-26T10:39:03.488608Z","iopub.status.idle":"2023-08-26T10:39:03.504847Z","shell.execute_reply.started":"2023-08-26T10:39:03.488582Z","shell.execute_reply":"2023-08-26T10:39:03.503606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_text_distribution(x_df, y_df, color, title, xaxis_text, yaxis_text):\n    \n    fig = px.bar(x=x_df, \n                y=y_df,\n                color=y_df,\n                text=y_df,\n                color_continuous_scale=color)\n\n    fig.update_layout(\n        title_text=title,\n        template='plotly_white',\n        xaxis=dict(\n            title=xaxis_text,\n        ),\n        yaxis=dict(\n            title=yaxis_text,\n        )\n    )\n\n    fig.update_traces(marker_line_color='black', \n                    marker_line_width=1.5, \n                    opacity=0.8)\n    \n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-26T10:39:03.506655Z","iopub.execute_input":"2023-08-26T10:39:03.507153Z","iopub.status.idle":"2023-08-26T10:39:03.52144Z","shell.execute_reply.started":"2023-08-26T10:39:03.507122Z","shell.execute_reply":"2023-08-26T10:39:03.520131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_wordcloud(freq_df, title, color):\n    \n    data = freq_df.set_index('Term').to_dict()['Frequency']\n    \n    plt.figure(figsize = (20,15))\n    wc = WordCloud(width=800, \n               height=400, \n               max_words=100,\n               colormap= color,\n               max_font_size=200,\n               min_font_size = 1 ,\n               random_state=8888, \n               background_color='white').generate_from_frequencies(data)\n    \n    plt.imshow(wc, interpolation='bilinear')\n    plt.title(title, fontsize=20)\n    plt.axis('off')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-26T10:39:03.522591Z","iopub.execute_input":"2023-08-26T10:39:03.52334Z","iopub.status.idle":"2023-08-26T10:39:03.536811Z","shell.execute_reply.started":"2023-08-26T10:39:03.523301Z","shell.execute_reply":"2023-08-26T10:39:03.536002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div id='2.2.2' style='background-color:#9FDFFF;padding:2px;border-radius:25px'>\n    <h4 style='font-family:Impact; color:black'>&nbsp 2.2.2 Positive Sentiment Text Distribution</h4>\n</div>","metadata":{}},{"cell_type":"code","source":"positive_words = preprocess(train_df.loc[train_df['Sentiment'] == 'Positive']['Content'])\npositive_words_df = create_freq_df(positive_words)\ntop10_positive_words = positive_words_df[:10]\n\nplot_text_distribution(top10_positive_words['Term'], top10_positive_words['Frequency'],\n                  'Greens', 'Top 10 Positive Sentiment Text Distribution', 'Text', 'Number of Texts')\ncreate_wordcloud(positive_words_df, 'Positive Sentiment Text Distribution', 'BuGn')","metadata":{"execution":{"iopub.status.busy":"2023-08-26T10:39:03.537988Z","iopub.execute_input":"2023-08-26T10:39:03.538306Z","iopub.status.idle":"2023-08-26T10:39:07.612913Z","shell.execute_reply.started":"2023-08-26T10:39:03.53828Z","shell.execute_reply":"2023-08-26T10:39:07.611967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div id='2.2.3' style='background-color:#9FDFFF;padding:2px;border-radius:25px'>\n    <h4 style='font-family:Impact; color:black'>&nbsp 2.2.3 Negative Sentiment Text Distribution</h4>\n</div>","metadata":{}},{"cell_type":"code","source":"negative_words = preprocess(train_df.loc[train_df['Sentiment'] == 'Negative']['Content'])\nnegative_words_df = create_freq_df(negative_words)\ntop10_negative_words = negative_words_df[:10]\n\nplot_text_distribution(top10_negative_words['Term'], top10_negative_words['Frequency'],\n                  'Reds', 'Top 10 Negative Sentiment Text Distribution', 'Text', 'Number of Texts')\ncreate_wordcloud(negative_words_df, 'Negative Sentiment Text Distribution', 'OrRd')","metadata":{"execution":{"iopub.status.busy":"2023-08-26T10:39:07.61385Z","iopub.execute_input":"2023-08-26T10:39:07.614152Z","iopub.status.idle":"2023-08-26T10:39:10.609378Z","shell.execute_reply.started":"2023-08-26T10:39:07.614126Z","shell.execute_reply":"2023-08-26T10:39:10.60828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div id='2.2.4' style='background-color:#9FDFFF;padding:2px;border-radius:25px'>\n    <h4 style='font-family:Impact; color:black'>&nbsp 2.2.4 Neutral Sentiment Text Distribution</h4>\n</div>","metadata":{}},{"cell_type":"code","source":"neutral_words = preprocess(train_df.loc[train_df['Sentiment'] == 'Neutral']['Content'])\nneutral_words_df = create_freq_df(neutral_words)\ntop10_neutral_words = neutral_words_df[:10]\n\nplot_text_distribution(top10_neutral_words['Term'], top10_neutral_words['Frequency'],\n                  'Greys', 'Top 10 Neutral Sentiment Text Distribution', 'Text', 'Number of Texts')\ncreate_wordcloud(neutral_words_df, 'Neutral Sentiment Text Distribution', 'binary_r')","metadata":{"execution":{"iopub.status.busy":"2023-08-26T10:39:10.610528Z","iopub.execute_input":"2023-08-26T10:39:10.611214Z","iopub.status.idle":"2023-08-26T10:39:14.408631Z","shell.execute_reply.started":"2023-08-26T10:39:10.611157Z","shell.execute_reply":"2023-08-26T10:39:14.407589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The interesting thing is that the <b>most frequent</b> word for all sentiments is <mark><b>game</b></mark>.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n# <div style=\"text-align: center; background-color: #00aced; font-family:Impact; color: white; padding: 14px; line-height: 1;border-radius:20px\">3. ML Pipeline Modelling ü§ñ</div>","metadata":{}},{"cell_type":"markdown","source":"In this section, I will <b>build a pipeline</b> to find out <mark>optimized parameters of TfidfVectorizer and logistic regression. </mark>","metadata":{}},{"cell_type":"code","source":"X_train = train_df['Content']\nX_test = test_df['Content']\ny_train = train_df['Sentiment']\ny_test = test_df['Sentiment']","metadata":{"execution":{"iopub.status.busy":"2023-08-26T10:39:14.410022Z","iopub.execute_input":"2023-08-26T10:39:14.410396Z","iopub.status.idle":"2023-08-26T10:39:14.416788Z","shell.execute_reply.started":"2023-08-26T10:39:14.410366Z","shell.execute_reply":"2023-08-26T10:39:14.415272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline = Pipeline([\n    ('tfidf_vect', TfidfVectorizer(stop_words='english')),\n    ('lr_clf', LogisticRegression(solver='liblinear'))\n])\n\nparams = {'tfidf_vect__ngram_range': [(1,1), (1,2), (1,3)],\n          'tfidf_vect__max_df': [0.5, 0.75, 1.0],\n          'lr_clf__C': [1, 5, 10]}\n\ngrid_cv_pipe = GridSearchCV(pipeline, param_grid=params, cv=3, scoring='accuracy', verbose=1)\ngrid_cv_pipe.fit(X_train, y_train)\nprint('Optimized Hyperparameters: ', grid_cv_pipe.best_params_)\n\npred = grid_cv_pipe.predict(X_test)\nprint('Optimized Accuracy Score: {0: .3f}'.format(accuracy_score(y_test, pred)))","metadata":{"execution":{"iopub.status.busy":"2023-08-26T10:39:14.418101Z","iopub.execute_input":"2023-08-26T10:39:14.418576Z","iopub.status.idle":"2023-08-26T10:46:50.391665Z","shell.execute_reply.started":"2023-08-26T10:39:14.418544Z","shell.execute_reply":"2023-08-26T10:46:50.390514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The optimized hyperparmeters are <b>{'lr_clf__C': 1, 'tfidf_vect__max_df': 0.5, 'tfidf_vect__ngram_range': (1, 3)}</b>, and the optimized accuracy of test dataset is <mark><b>96.8%</b></mark>.","metadata":{}}]}